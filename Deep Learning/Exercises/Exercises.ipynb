{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "**Sigmoid neurons simulating perceptrons, part I**<br />\n",
    "*Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, $c>0$. Show that the behaviour of the network doesn't change.*\n",
    "\n",
    "The perceptron rule can be written as \n",
    "\n",
    "![title](perceptron.png)\n",
    "\n",
    "Multiplying this equation with a positive constant, $c>0$, gives the following equation\n",
    "\n",
    "$$z = c(wx + b) $$\n",
    "From this it is evident that multiplying with a positive constant doesn't change the sign of the equation it only scales the output.\n",
    "\n",
    "### 2\n",
    "**Sigmoid neurons simulating perceptrons, part II**<br />\n",
    "*Suppose we have the same setup as the last problem - a network of perceptrons. Suppose also that the overall input to the network of perceptrons has been chosen. We won't need the actual input value, we just need the input to have been fixed. Suppose the weights and biases are such that $wx+b \\neq 0$ for the input $x$ to any particular perceptron in the network. Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant $c>0$. Show that in the limit as $c \\rightarrow \\infty$ the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when $wx+b=0$.*\n",
    " \n",
    "Multipliying $z = wx+b$, with a constant $c\\rightarrow\\infty$ will make the sigmoid function defined as\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{(1+e^{-z})}$$,\n",
    "\n",
    "going towards one since $e^{-z}\\rightarrow0$.\n",
    "\n",
    "For $z=0$, $\\sigma(z) = 1/2$ and which will be between one and zero and thus it cannot classify the binary problem.\n",
    "\n",
    "\n",
    "### 3\n",
    "*An extreme version of gradient descent is to use a mini-batch size of just 1. In online learning, a neural network learns from just one training input at a time (just as human beings do). Name one advantage and one disadvantage of online learning, compared to stochastic gradient descent with a mini-batch size of, say, 20.*\n",
    "\n",
    "Changing the mini-batch to size 1 will lead to a lower computational cost, but the classifier will become more prone to outliers and thus convergance to a minimum might be slower.\n",
    "\n",
    "\n",
    "### 4\n",
    "*Try creating a network with just two layers - an input and an output layer, no hidden layer - with 784 and 10 neurons, respectively. Train the network using stochastic gradient descent. What classification accuracy can you achieve?*\n",
    "\n",
    "Implementing a network with the specifications from above results in a the following classification.\n",
    "![title](exercisePlot.png)\n",
    "\n",
    "\n",
    "### 5\n",
    "*Write out $a'=\\sigma(wa+b)$ in component form, and verify that it gives the same result as the equation below for computing the output of a sigmoid neuron.*\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{1}{1+exp(-\\Sigma_jw_jx_j-b)} $$\n",
    "\n",
    "We know that the dot product of two vectors is defined as \n",
    "$$wa = \\Sigma_jw_ja $$,\n",
    "inserting this in the equation from question **2** gives the desired result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
